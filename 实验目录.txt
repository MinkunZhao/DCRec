1.主方法主实验——表格形式
	指标：Recall@[5,10,20]，NDCG@[5,10,20]；
	用到的backbone模型：lightgcn、sgl、simgcl、gccf、
			    人大的ncl、清华的direct_au、adagcl。
	每个都计算相对baseline所提升的幅度。

2.（对比RLMrec，LLMRec,RGCL等）验证在均使用对比学习的前提下，大部分时候，
		先用双通道信息传播再聚合，比先聚合再用单通道传播，的效果更好——表格形式

3.消融实验——柱状图/条形图形式
	（3.1）验证选用GAT作为信息传播的神经网络的有效性
		（3.1.1）把GAT换成GCN
		（3.1.2）把GAT换成GraphSage
		（3.1.3）把GAT换成Graph-Transformer
		（3.1.4）our's 主方法
	（3.2）验证整体模型的各个组件的有效性
		（3.2.1）验证协同关系结构侧的GCN的必要性（w/o general cf）
		（3.2.2）验证门控融合机制的有效性（w/o gate fusion,即暴力直接拼接俩模态的embedding）
		（3.2.3）验证LLM提供个门控权重label的有效性（w/o LLM's supervision, random label）
		（3.2.4）our's 主方法
	（3.3）验证，在语义图生成与构建的环节，若把语义图构建的逻辑改得更复杂，并不会使其表现更好，反而使其表现变差
			（因为在语义图构建环节搞复杂，违背了"信号密度最大化，噪声干扰最小化"的原则）
		（3.3.1）把semantic graph中的u->i单向边改为u<->i的双向边
		（3.3.2）除了“u-i”的边，还额外加入“u-u”边和“i-i”边
		（3.3.3）选取topk时，除了直接比较emb的相似度，还引入TF-IDF，来综合决定相似度
		（3.3.4）our's 主方法

4.验证用于生成语义图的文本信息很重要——折线图形式
	（4.1）随机删文本
		（4.1.1）our's 原
		（4.1.2）随机删除10%文本
		（4.1.3）随机删除30%文本
		（4.1.4）随机删除50%文本
		（4.1.5）随机删除70%文本
	（4.2）随机加噪声文本
		（4.2.1）our's 原
		（4.2.2）随机加10%噪声
		（4.2.3）随机加20%噪声
		（4.2.4）随机加30%噪声
		（4.2.5）随机加40%噪声

5.超参数分析实验——
	（5.1）调语义图构建里的topk
	（5.2）调embedding_size
	（5.3）调GAT中注意力头的数量
	（5.4）GAT中edge_dim（关注几跳，高阶/低阶语义推理）

6.Case Study（这部分我没啥思路，不知道怎么做以及做成什么形式的）